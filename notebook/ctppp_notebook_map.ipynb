{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the CT PPP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries. To avoid truncating the number of rows displayed in output, the max_rows below can be set to 'None'. Because output can be over 60K rows, this limit is useful to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import geopandas as gpd\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = \"{:,.0f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the three JSON files that contain the CT PPP data. Unless the JSON files are in the same working directory as this notebook when working locally on your computer, it will necessary to use full path names for the files. <br><br>If using Google Colab, first upload these JSON files to Colab by clicking the folder icon at the notebook's sidebar and then clicking the upload button. Then preface the JSON filenames below with '/content/', so that the first filename below is '/content/ctppp_small_063020.json' instead. <br><br>Here we assign <\\\\$150K loans to the small_df dataframe, the >\\\\$150K loans to the large_df dataframe, and the composite dataset to the total_df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = pd.read_json('ctppp_small_063020.json', dtype={'Zip': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_df = pd.read_json('ctppp_large_063020.json', dtype={'Zip': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_json('ctppp_total_063020.json', dtype={'Zip': 'str'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple approach for creating point estimates of the >\\\\$150K loans is to assign the same dollar value for all loans in the same dollar range. E.g., \\\\$7.5M for all \\\\$5M-10M loans. This is not an actual estimate per se, but a standardization of the larger loan values. For the sake of simplicity, this approach is used below to illustrate aggregrate loan dollar amounts. This standardization uses the following steps:\n",
    "1. Use the reported SBA total loan dollar amount for CT and subtract out deleted loans from this SBA total. In subtracting out the deleted loans, for <\\\\$150K loans, use the actual loan amount reported; for >\\\\$150K loans, use the midpoint of the dollar range for that loan. E.g., \\\\$250K for \\\\$150K-\\\\$350K loans.\n",
    "2. Compute a percentage of the dollar range that will be added to the loan mininum to give the standardized loan value. Use the same percentage for all loan ranges. E.g., 50% would yield the midpoint for all the loan ranges. In the initial tranche of data provided by the SBA, 35.425% was deemed appropriate after cleaning the data and subtracting out deleted items.\n",
    "3. Follow this approach to create another standardized column of loan amounts in the large and total loan dataframes. Actual reported loan amounts for the <\\\\$150K loans are still always used in this new column, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization_percentage = 0.35425\n",
    "large_df['StandardLoanAmount'] = (large_df['LoanMin']+(large_df['LoanMax']-large_df['LoanMin'])*standardization_percentage)\n",
    "total_df['StandardLoanAmount'] = (total_df['LoanMin']+(total_df['LoanMax']-total_df['LoanMin'])*standardization_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these standardized loan amounts, it is easier to illustrate aggregate relationships in the data. The CT PPP data is now fully available in the notebook for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python library used here for mapping is Geopandas. First load Connecticut's zip code information from the GEOJSON file. Then merge this geographical information with data from the one of the existing dataframes to produce state maps that illustrate the data spatially over zip codes. The GEOJSON file is assumed to be in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 'ct_zip_code_boundaries.geojson'\n",
    "map_df = gpd.read_file(fp)\n",
    "map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.rename(columns={'zcta5ce10': 'Zip'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the map, prepare another dataframe used to put the major cities of Connecticut onto the state map. The JSON file containing these coordinates is assumed to be in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = pd.read_json('cities_for_map.json',dtype={'latitude': 'float','longitude': 'float'})\n",
    "cities_df['coordinates'] = list(zip(cities_df.longitude, cities_df.latitude))\n",
    "\n",
    "from shapely.geometry import Point\n",
    "cities_df['coordinates'] = cities_df['coordinates'].apply(Point)\n",
    "cities_map_df = gpd.GeoDataFrame(cities_df, geometry='coordinates')\n",
    "cities_map_df.crs= 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the map, filling the zip codes that have null values with zero to ensure their zip code polygons are plotted. Find the largest number of jobs in a zip code in order to manually label the color bar. The color bar coordinates have also been determined manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.groupby('Zip')['JobsRetained'].sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_jobs_amount_by_zip = total_df.groupby('Zip')['JobsRetained'].sum()\n",
    "map_total_jobs_amount_df = pd.merge(map_df, total_jobs_amount_by_zip, on='Zip', how='left')\n",
    "map_total_jobs_amount_df['JobsRetained'].fillna(0, inplace=True)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (11.15,7.75)\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.set_title(label='Total retained jobs across CT from PPP lending',\n",
    "                     fontdict={'fontsize': 18,'fontweight': 'bold'})\n",
    "\n",
    "vmin = 0\n",
    "vmax = 13\n",
    "cax = fig.add_axes([0.60, 0.22, 0.25, 0.015])\n",
    "\n",
    "cb = fig.colorbar(label='jobs by zip code (000s)', shrink = 0.25, ax=ax, cax=cax, orientation='horizontal', \n",
    "             mappable=plt.cm.ScalarMappable(cmap='Blues',norm=plt.Normalize(vmin=vmin,vmax=vmax)))\n",
    "map_total_jobs_amount_df.plot(column='JobsRetained', cmap='Blues', edgecolor='gray',\n",
    "                 linewidth=0.25, ax=ax)\n",
    "\n",
    "cities_map_df.plot(ax=ax, color='black', alpha = 0.5)\n",
    "\n",
    "for x, y, label in zip(cities_map_df.coordinates.x, cities_map_df.coordinates.y, cities_map_df.city):\n",
    "    if label in ['Stamford', 'Bridgeport', 'New Haven', 'Hartford', 'Waterbury']:\n",
    "        ax.annotate(label, xy=(x,y), xytext=(4,4), textcoords='offset points', size=12)\n",
    "    else:\n",
    "        ax.annotate(label, xy=(x,y), xytext=(4,4), textcoords='offset points', size=10)\n",
    "        \n",
    "plt.savefig('map_total_jobs_by_zip.png', bbox_inches='tight', dpi=600, height=7.75, width=11.15, units='in', transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
